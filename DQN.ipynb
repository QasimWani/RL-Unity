{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train an RL agent using DQN architecture in a Unity environment (bananaModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque, defaultdict\n",
    "import time\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from dqnetwork import DQNetwork\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Banana.app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action space:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "### State space:\n",
    "- `37` - dimensions.\n",
    "- some samples include the agent's velocity.\n",
    "- ray-based perception in the forward direction of the agent.\n",
    "\n",
    "### Reward:\n",
    "\n",
    "- `+1` - Yellow Banana collected.\n",
    "- `-1` - Blue Banana collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States look like: [1.         0.         0.         0.         0.84408134 0.\n",
      " 0.         1.         0.         0.0748472  0.         1.\n",
      " 0.         0.         0.25755    1.         0.         0.\n",
      " 0.         0.74177343 0.         1.         0.         0.\n",
      " 0.25854847 0.         0.         1.         0.         0.09355672\n",
      " 0.         1.         0.         0.         0.31969345 0.\n",
      " 0.        ]\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "# state = env_info.vector_observations[0]            # get the current state\n",
    "# score = 0                                          # initialize the score\n",
    "# i = 0\n",
    "# while True:\n",
    "#     i+=1\n",
    "#     action = np.random.randint(action_size)        # select an action\n",
    "#     env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "#     next_state = env_info.vector_observations[0]   # get the next state\n",
    "#     reward = env_info.rewards[0]                   # get the reward\n",
    "#     done = env_info.local_done[0]                  # see if episode has finished\n",
    "#     score += reward                                # update the score\n",
    "#     state = next_state                             # roll over the state to next time step\n",
    "#     if(reward != 0):\n",
    "#         print(reward)\n",
    "#     if done:                                       # exit loop if episode finished\n",
    "#         break\n",
    "    \n",
    "# print(\"Score: {}\".format(score))\n",
    "# print(\"iterations:\",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (37x128) -> (128x64) -> (64x32) -> (32x4)\n",
    "input_features = [state_size, 128, 64, 32]\n",
    "output_features = [128, 64, 32, action_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQNetwork(input_features, output_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define the agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINE CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate \n",
    "UPDATE_EVERY = 4        # how often to update the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import deque, defaultdict\n",
    "import torch.optim as optim\n",
    "from dqnetwork import DQNetwork\n",
    "import random\n",
    "from buffer import ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"Defines the agent class for DQN using Double Q-learning and Prioritized Experience Replay architecture\"\"\"\n",
    "    def __init__(self, state_size=37, action_size=4, gamma=0.99, lr=0.001, update_every=5):\n",
    "        \"\"\"\n",
    "        Initializes the model.\n",
    "        ----\n",
    "        @param:\n",
    "        1. state_size: size of input # of states.\n",
    "        2. action_size: size of # of actions.\n",
    "        3. gamma: discounted return rate.\n",
    "        4. lr: learning rate for the model.\n",
    "        5. update_every: update target_model every X time-steps.\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.input_features = [state_size, 128, 64, 32]\n",
    "        self.output_features = [128, 64, 32, action_size]\n",
    "        self.gamma = gamma\n",
    "        #Q-network : defines the 2 DQN (using doubling Q-learning architecture via fixed Q target)\n",
    "        self.qnetwork_local = DQNetwork(self.input_features, self.output_features)\n",
    "        self.qnetwork_target = DQNetwork(self.input_features, self.output_features)\n",
    "        #define the optimizer\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=lr)\n",
    "        \n",
    "        #replay memory\n",
    "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE)\n",
    "        self.update_every = update_every\n",
    "        self.target_update_counter = 0\n",
    "        \n",
    "    def step(self, transition):\n",
    "        \"\"\"Performs forward pass of the tranisition\n",
    "        @param:\n",
    "        1. transition : (tuple) state, action, reward, next_state, done\n",
    "        \"\"\"\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(transition)\n",
    "        self.target_update_counter = (self.target_update_counter + 1) % self.update_every\n",
    "        \n",
    "        #Update target network to local network\n",
    "        if(self.target_update_counter == 0):\n",
    "            #primary condition to check if len(buffer) > batch_size\n",
    "            if(len(self.memory) > BATCH_SIZE):\n",
    "                experiences = self.memory.sample()\n",
    "                self.train(experiences)\n",
    "    \n",
    "    def get_action(self, state, eps=0.0):\n",
    "        \"\"\"\n",
    "        Determines the action to perform based on epsilon-greedy method\n",
    "        @param:\n",
    "        1. state - list of current observations to determine an action for\n",
    "        2. eps - value for epsilon, stochastic measure.\n",
    "        @return:\n",
    "        - action = action chosen by either equiprobably π or using Q-table\n",
    "        \"\"\"\n",
    "        print(state.shape)\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_val = self.qnetwork_local(state)\n",
    "        print(action_val)\n",
    "        self.qnetwork_local.train()#train local network\n",
    "        \n",
    "        #Epsilon-greedy selection\n",
    "        if(random.random() > eps):#exploit\n",
    "            return np.argmax(action_val.cpu().data.numpy())\n",
    "        \n",
    "        return random.choice(np.arange(self.action_size))#explore\n",
    "    \n",
    "    def train(self, experiences):\n",
    "        \"\"\"\n",
    "        Train the model.\n",
    "        @param:\n",
    "        1. experiences: (Tuple[torch.Variable]) (s,a,r,s',done)\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, done = experiences\n",
    "        \n",
    "        #Implement SGD using Adam as regularizer\n",
    "        Q_targets_next = self.qnetwork_target(next_states.float().unsqueeze(0)).detach().max(1)[0].unsqueeze(0)\n",
    "        Q_targets = rewards + (self.gamma * Q_targets_next.T * (~done))\n",
    "        print(states.shape, actions.T.shape)\n",
    "        Q_expected = self.qnetwork_local(states.float().unsqueeze(-2)).T.detach()\n",
    "        print(Q_targets.shape, Q_targets_next.shape, Q_expected.shape)\n",
    "        #set loss as mse.\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        #Update target network using soft update\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target)\n",
    "    \n",
    "    def soft_update(self, local_model, target_model):\n",
    "        \"\"\"\n",
    "        Update target network to local network using a soft update param, τ.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        ------\n",
    "        @param:\n",
    "        1. local_model: (DQNetwork) local network model (weights will be copied from)\n",
    "        2. target_model: (DQNetwork) target network model (weights will be copied into)\n",
    "        \"\"\"\n",
    "        \n",
    "        target_model = TAU*local_model + (1 - TAU) * target_model\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the replay buffer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epsilon_i(num_episode, epsilon_min=0.01, evaluate=True):\n",
    "    \"\"\"Gets the epsilon value given the current episode number\"\"\"\n",
    "    if(evaluate):\n",
    "        epsilon = max(1/num_episode, epsilon_min)\n",
    "    return epsilon_min #change this by using a hypertuned value for epsilon when testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []  # List with all scores per episode\n",
    "scores_100_mean = 0 # Mean score over the last 100 episodes\n",
    "\n",
    "NUM_EPISODES = 20\n",
    "CRIT_SOLVED = 13 #How many Bananas must be collected to succeed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(37,)\n",
      "tensor([[ 0.0186, -0.0932,  0.0842,  0.1311]])\n",
      "2\n",
      "(37,)\n",
      "tensor([[ 0.0295, -0.0792,  0.0757,  0.1487]])\n",
      "3\n",
      "(37,)\n",
      "tensor([[ 0.0277, -0.1013,  0.0529,  0.1842]])\n",
      "4\n",
      "(37,)\n",
      "tensor([[ 0.0049, -0.1065,  0.0891,  0.1052]])\n",
      "5\n",
      "(37,)\n",
      "tensor([[ 0.0168, -0.0630,  0.0722,  0.1730]])\n",
      "6\n",
      "(37,)\n",
      "tensor([[-0.0018, -0.0732,  0.0518,  0.1693]])\n",
      "7\n",
      "(37,)\n",
      "tensor([[ 0.0393, -0.0982,  0.0783,  0.1449]])\n",
      "8\n",
      "(37,)\n",
      "tensor([[-0.0218, -0.0380,  0.0845,  0.1548]])\n",
      "9\n",
      "(37,)\n",
      "tensor([[-0.0041, -0.0621,  0.0894,  0.1437]])\n",
      "10\n",
      "(37,)\n",
      "tensor([[ 0.0090, -0.0098,  0.0725,  0.1613]])\n",
      "11\n",
      "(37,)\n",
      "tensor([[ 0.0380, -0.0634,  0.0665,  0.1595]])\n",
      "12\n",
      "(37,)\n",
      "tensor([[ 0.0349, -0.0684,  0.0846,  0.1663]])\n",
      "13\n",
      "(37,)\n",
      "tensor([[ 0.0297, -0.1097,  0.0769,  0.1058]])\n",
      "14\n",
      "(37,)\n",
      "tensor([[ 0.0405, -0.0633,  0.0626,  0.1605]])\n",
      "15\n",
      "(37,)\n",
      "tensor([[-0.0020, -0.1205,  0.0585,  0.2168]])\n",
      "16\n",
      "(37,)\n",
      "tensor([[-0.0062, -0.0857,  0.0732,  0.1328]])\n",
      "17\n",
      "(37,)\n",
      "tensor([[ 0.0350, -0.0764,  0.0703,  0.1502]])\n",
      "18\n",
      "(37,)\n",
      "tensor([[ 0.0298, -0.0660,  0.1072,  0.1435]])\n",
      "19\n",
      "(37,)\n",
      "tensor([[ 0.0667, -0.1176,  0.1021,  0.1316]])\n",
      "20\n",
      "(37,)\n",
      "tensor([[ 0.0282, -0.0608,  0.0660,  0.1435]])\n",
      "21\n",
      "(37,)\n",
      "tensor([[ 0.0229, -0.0443,  0.1121,  0.1738]])\n",
      "22\n",
      "(37,)\n",
      "tensor([[ 0.0266, -0.0396,  0.0648,  0.1664]])\n",
      "23\n",
      "(37,)\n",
      "tensor([[ 0.0232, -0.0982,  0.0923,  0.1367]])\n",
      "24\n",
      "(37,)\n",
      "tensor([[ 0.0041, -0.0947,  0.0898,  0.1542]])\n",
      "25\n",
      "(37,)\n",
      "tensor([[ 0.0113, -0.1060,  0.0869,  0.1373]])\n",
      "26\n",
      "(37,)\n",
      "tensor([[ 0.0156, -0.0806,  0.0726,  0.1448]])\n",
      "27\n",
      "(37,)\n",
      "tensor([[ 0.0469, -0.0915,  0.0952,  0.1432]])\n",
      "28\n",
      "(37,)\n",
      "tensor([[ 0.0247, -0.1400,  0.0761,  0.0981]])\n",
      "29\n",
      "(37,)\n",
      "tensor([[ 0.0431, -0.0753,  0.0608,  0.1544]])\n",
      "30\n",
      "(37,)\n",
      "tensor([[ 0.0448, -0.0850,  0.0595,  0.1552]])\n",
      "31\n",
      "(37,)\n",
      "tensor([[ 0.0082, -0.0528,  0.1159,  0.1122]])\n",
      "32\n",
      "(37,)\n",
      "tensor([[ 0.0270, -0.0964,  0.0490,  0.1674]])\n",
      "33\n",
      "(37,)\n",
      "tensor([[ 0.0042, -0.0780,  0.0938,  0.1254]])\n",
      "34\n",
      "(37,)\n",
      "tensor([[ 0.0151, -0.0560,  0.0624,  0.1871]])\n",
      "35\n",
      "(37,)\n",
      "tensor([[ 0.0194, -0.0319,  0.0917,  0.1454]])\n",
      "36\n",
      "(37,)\n",
      "tensor([[ 0.0503, -0.0900,  0.0614,  0.1723]])\n",
      "37\n",
      "(37,)\n",
      "tensor([[ 0.0263, -0.1023,  0.0872,  0.1438]])\n",
      "38\n",
      "(37,)\n",
      "tensor([[ 0.0340, -0.1078,  0.0730,  0.1383]])\n",
      "39\n",
      "(37,)\n",
      "tensor([[ 0.0096, -0.1155,  0.0806,  0.1247]])\n",
      "40\n",
      "(37,)\n",
      "tensor([[ 0.0133, -0.0891,  0.0839,  0.1170]])\n",
      "41\n",
      "(37,)\n",
      "tensor([[ 0.0009, -0.0877,  0.0652,  0.1802]])\n",
      "42\n",
      "(37,)\n",
      "tensor([[-0.0028, -0.0886,  0.0990,  0.1321]])\n",
      "43\n",
      "(37,)\n",
      "tensor([[ 0.0069, -0.0707,  0.1201,  0.1154]])\n",
      "44\n",
      "(37,)\n",
      "tensor([[ 0.0026, -0.0600,  0.0824,  0.1541]])\n",
      "45\n",
      "(37,)\n",
      "tensor([[ 0.0123, -0.0823,  0.0881,  0.1509]])\n",
      "46\n",
      "(37,)\n",
      "tensor([[ 0.0473, -0.0591,  0.0620,  0.1844]])\n",
      "47\n",
      "(37,)\n",
      "tensor([[-0.0118, -0.0770,  0.1284,  0.1062]])\n",
      "48\n",
      "(37,)\n",
      "tensor([[ 0.0062, -0.0844,  0.0698,  0.1508]])\n",
      "49\n",
      "(37,)\n",
      "tensor([[ 0.0064, -0.0478,  0.0676,  0.1579]])\n",
      "50\n",
      "(37,)\n",
      "tensor([[ 0.0008, -0.0553,  0.0947,  0.1292]])\n",
      "51\n",
      "(37,)\n",
      "tensor([[ 0.0431, -0.0364,  0.0670,  0.1754]])\n",
      "52\n",
      "(37,)\n",
      "tensor([[ 0.0297, -0.0678,  0.0975,  0.1396]])\n",
      "53\n",
      "(37,)\n",
      "tensor([[ 0.0263, -0.0738,  0.0937,  0.1260]])\n",
      "54\n",
      "(37,)\n",
      "tensor([[ 0.0362, -0.0922,  0.0500,  0.1469]])\n",
      "55\n",
      "(37,)\n",
      "tensor([[-0.0066, -0.0777,  0.1047,  0.1168]])\n",
      "56\n",
      "(37,)\n",
      "tensor([[ 0.0165, -0.0612,  0.0844,  0.1427]])\n",
      "57\n",
      "(37,)\n",
      "tensor([[ 0.0117, -0.0864,  0.0583,  0.1922]])\n",
      "58\n",
      "(37,)\n",
      "tensor([[ 0.0120, -0.1320,  0.0992,  0.1069]])\n",
      "59\n",
      "(37,)\n",
      "tensor([[-0.0027, -0.0636,  0.0837,  0.1377]])\n",
      "60\n",
      "(37,)\n",
      "tensor([[ 0.0534, -0.0991,  0.0335,  0.1742]])\n",
      "61\n",
      "(37,)\n",
      "tensor([[ 0.0477, -0.0768,  0.0657,  0.1521]])\n",
      "62\n",
      "(37,)\n",
      "tensor([[ 0.0189, -0.1105,  0.0630,  0.1598]])\n",
      "63\n",
      "(37,)\n",
      "tensor([[ 0.0384, -0.0941,  0.0864,  0.1552]])\n",
      "64\n",
      "(37,)\n",
      "tensor([[ 0.0562, -0.0608,  0.0816,  0.1526]])\n",
      "65\n",
      "(37,)\n",
      "tensor([[ 0.0073, -0.0630,  0.0762,  0.1551]])\n",
      "torch.Size([64, 37]) torch.Size([64])\n",
      "torch.Size([4, 1, 64]) torch.Size([1, 1, 4]) torch.Size([4, 1, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-178-212068df461b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_done\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m                  \u001b[0;31m# see if episode has finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtransition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#set transition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransition\u001b[0m\u001b[0;34m)\u001b[0m                         \u001b[0;31m# step into the next state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m                                \u001b[0;31m# update the score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m                             \u001b[0;31m# roll over the state to next time step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-173-50233688ff15>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, transition)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-173-50233688ff15>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, experiences)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_expected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/drlnd/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/drlnd/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NUM_EPISODES + 1):\n",
    "\n",
    "    env_info = env.reset(train_mode=True)[brain_name]  # reset the environment\n",
    "    state = env_info.vector_observations[0]            # get the current state\n",
    "    score = 0     # initialize the score\n",
    "    i = 0\n",
    "    while True:\n",
    "        i += 1\n",
    "        print(i)\n",
    "        epsilon = get_epsilon_i(epoch)\n",
    "        action = agent.get_action(state, epsilon)        # select an action\n",
    "        env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "        next_state = env_info.vector_observations[0]   # get the next state\n",
    "        reward = env_info.rewards[0]                   # get the reward\n",
    "        done = env_info.local_done[0]                  # see if episode has finished\n",
    "        transition = (state, action, reward, next_state, done) #set transition\n",
    "        agent.step(transition)                         # step into the next state\n",
    "        score += reward                                # update the score\n",
    "        state = next_state                             # roll over the state to next time step\n",
    "        if done:                                       # exit loop if episode finished\n",
    "            break\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = [-0.0978,  0.2430, -0.0965,  0.0018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition = (state, action, 0.0, state, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer.add(transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    buffer.add(transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d,e = buffer.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "qn = DQNetwork(input_features, output_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = qn(a.float()).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = torch.tensor(np.random.rand(3,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 4]), torch.Size([4, 1, 64]))"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, x.unsqueeze(-2).T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_targets_next = qn(a.float().unsqueeze(0)).detach().max(1)[0].unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0539, -0.0262, -0.0098,  ...,  0.1301,  0.1375,  0.0429],\n",
       "         [ 0.0349, -0.0146, -0.0793,  ...,  0.0408, -0.0905,  0.1391],\n",
       "         [ 0.1244, -0.1340,  0.0557,  ...,  0.1120,  0.1314, -0.1232],\n",
       "         ...,\n",
       "         [ 0.0220, -0.0387, -0.1189,  ..., -0.0032, -0.1463, -0.0024],\n",
       "         [ 0.0111, -0.0915, -0.0194,  ..., -0.0254, -0.0364, -0.0644],\n",
       "         [ 0.1281,  0.0151, -0.0458,  ...,  0.0606, -0.0779, -0.1158]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0425,  0.1634,  0.0830,  0.1200, -0.0655,  0.1611,  0.0110,  0.1185,\n",
       "         -0.0874, -0.1617, -0.0543,  0.1095,  0.0146,  0.0081, -0.1641, -0.0321,\n",
       "         -0.0520, -0.0183, -0.1345,  0.1421, -0.0356,  0.0940,  0.1099, -0.0594,\n",
       "         -0.0359,  0.1353, -0.0715,  0.1567,  0.1133,  0.0945,  0.0994,  0.0249,\n",
       "         -0.0744, -0.0306, -0.0724,  0.0480, -0.0824, -0.0948,  0.0127, -0.0121,\n",
       "          0.0019,  0.0223, -0.1622, -0.0886,  0.0951,  0.0889, -0.1583,  0.0644,\n",
       "         -0.0203, -0.0820,  0.1122, -0.1101, -0.0151, -0.0187,  0.1610, -0.1012,\n",
       "          0.1433,  0.0178,  0.0310, -0.1025, -0.1371, -0.1583,  0.0512, -0.0586,\n",
       "         -0.1145, -0.0878,  0.0725,  0.0939, -0.0818, -0.0886, -0.1017, -0.0199,\n",
       "          0.1551, -0.0982,  0.0066, -0.1099,  0.0924, -0.0695, -0.1193,  0.0268,\n",
       "         -0.0146,  0.1435,  0.0729, -0.1026,  0.1050,  0.1604, -0.0717, -0.0515,\n",
       "          0.0782,  0.0264,  0.0002,  0.0835, -0.0437,  0.0839,  0.0329, -0.0830,\n",
       "          0.1181,  0.0221, -0.0744,  0.0778,  0.0758,  0.0960,  0.0527, -0.1591,\n",
       "          0.0864,  0.0716, -0.1561, -0.0527, -0.1459, -0.0637,  0.0209, -0.1553,\n",
       "          0.0681, -0.1177,  0.0582,  0.0451, -0.1588,  0.1222, -0.0730,  0.1370,\n",
       "          0.0372, -0.0815,  0.0195,  0.0359, -0.0948, -0.0655, -0.1077, -0.1587],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0054,  0.0494,  0.0768,  ..., -0.0736, -0.0562,  0.0043],\n",
       "         [ 0.0048, -0.0220, -0.0346,  ..., -0.0854, -0.0445, -0.0847],\n",
       "         [ 0.0159,  0.0502,  0.0010,  ..., -0.0816,  0.0096, -0.0221],\n",
       "         ...,\n",
       "         [-0.0883,  0.0535,  0.0821,  ..., -0.0651,  0.0351,  0.0403],\n",
       "         [ 0.0109, -0.0598,  0.0726,  ...,  0.0232,  0.0505,  0.0731],\n",
       "         [-0.0758, -0.0843,  0.0721,  ...,  0.0549,  0.0883,  0.0071]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0629,  0.0793,  0.0225,  0.0319,  0.0158, -0.0180,  0.0663,  0.0745,\n",
       "         -0.0721,  0.0579, -0.0863, -0.0696, -0.0617, -0.0161, -0.0110, -0.0680,\n",
       "         -0.0624, -0.0761, -0.0831,  0.0191, -0.0632, -0.0502,  0.0757,  0.0163,\n",
       "         -0.0657,  0.0770,  0.0410,  0.0865,  0.0361,  0.0474,  0.0421, -0.0864,\n",
       "         -0.0356,  0.0262, -0.0665,  0.0490, -0.0072, -0.0020,  0.0335, -0.0035,\n",
       "          0.0831, -0.0035, -0.0512,  0.0663, -0.0495,  0.0685, -0.0398,  0.0524,\n",
       "         -0.0035,  0.0446,  0.0331,  0.0750, -0.0705, -0.0535,  0.0760, -0.0082,\n",
       "         -0.0614,  0.0411,  0.0619,  0.0540, -0.0727,  0.0226,  0.0744,  0.0100],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.1121, -0.0458,  0.0305,  ...,  0.1137,  0.0035,  0.1211],\n",
       "         [ 0.0155, -0.0771, -0.0917,  ...,  0.0855, -0.0821,  0.0054],\n",
       "         [ 0.1133,  0.0980,  0.0454,  ..., -0.0569,  0.0282,  0.0090],\n",
       "         ...,\n",
       "         [ 0.0448,  0.0753,  0.0618,  ...,  0.0294, -0.0102, -0.0609],\n",
       "         [ 0.1109,  0.0594,  0.0804,  ...,  0.0648,  0.0671,  0.0170],\n",
       "         [-0.1188,  0.0026, -0.0777,  ...,  0.0636,  0.1068, -0.0086]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.1098,  0.1011, -0.0629, -0.0705,  0.0774, -0.1155, -0.0710,  0.0578,\n",
       "         -0.0157, -0.0745, -0.1106, -0.0457,  0.0760, -0.0486,  0.0525,  0.0481,\n",
       "          0.0379, -0.0432, -0.0188,  0.0875, -0.1121,  0.0718,  0.0798,  0.0165,\n",
       "          0.1072, -0.0187, -0.1233, -0.0094,  0.0423, -0.0531, -0.1127, -0.0950],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.1163,  0.0533,  0.0565, -0.1657,  0.0141,  0.0278, -0.1614, -0.0748,\n",
       "          -0.0175, -0.1711, -0.1255,  0.0612,  0.1437,  0.1451, -0.1687, -0.0660,\n",
       "           0.0161,  0.1260,  0.0645, -0.0292,  0.0689,  0.0750,  0.0762,  0.1476,\n",
       "           0.0476,  0.0374,  0.0884,  0.1085,  0.0026,  0.0472,  0.1366,  0.0350],\n",
       "         [ 0.1367,  0.0461, -0.0450, -0.0457, -0.1182,  0.1566,  0.1658, -0.1205,\n",
       "          -0.1403, -0.1609, -0.1720,  0.0313,  0.1493, -0.0318,  0.1650,  0.0754,\n",
       "          -0.1105,  0.0681,  0.0992,  0.1683,  0.1481, -0.1121,  0.0612,  0.0009,\n",
       "          -0.0650,  0.0397, -0.0764, -0.1368,  0.0475,  0.0674, -0.0122, -0.0164],\n",
       "         [-0.0832,  0.0272,  0.1635,  0.1403, -0.0724, -0.1165,  0.1298, -0.0038,\n",
       "           0.1247, -0.1375, -0.0040,  0.1089,  0.0282, -0.0096,  0.0230, -0.1696,\n",
       "           0.0604, -0.1283, -0.0073, -0.1747,  0.1500, -0.1279,  0.1502,  0.0748,\n",
       "           0.0608, -0.1008,  0.0703, -0.1646, -0.1593, -0.0137,  0.0641, -0.0040],\n",
       "         [-0.0636, -0.0371,  0.0783,  0.1260,  0.1081, -0.1322, -0.0522, -0.0120,\n",
       "           0.0498, -0.1344, -0.1039, -0.1185, -0.1761, -0.0581, -0.0851,  0.1332,\n",
       "           0.0553,  0.1711,  0.1584, -0.0568, -0.0558, -0.0943, -0.0798, -0.0597,\n",
       "          -0.1588,  0.1606,  0.1032, -0.0800,  0.0735,  0.0213, -0.0535,  0.0138]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.1384, -0.1420, -0.1521,  0.0582], requires_grad=True)]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(qn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
